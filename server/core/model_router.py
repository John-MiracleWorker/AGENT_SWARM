"""
Model Router â€” Multi-provider LLM client with role-based routing.
Routes agents to the best available model based on their role.
Orchestrator â†’ Gemini (smart planning), Developers â†’ Groq (fast coding).
Falls back across providers when rate limits are hit.
Backward-compatible with the old GeminiClient interface.
"""

import asyncio
import json
import time
import logging
from dataclasses import dataclass
from typing import Optional

from server.core.llm_providers import (
    GeminiProvider,
    GroqProvider,
    OpenAIProvider,
    ModelState,
)

logger = logging.getLogger(__name__)


class BudgetExhaustedError(Exception):
    """Raised when the API budget limit is reached."""
    pass


# â”€â”€â”€ Model Definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MODELS = [
    # Gemini models (paid)
    {"name": "gemini-3-pro-preview",    "provider": "gemini",  "rpm": 5,  "cost_in": 1.25, "cost_out": 10.00, "tier": "premium"},
    {"name": "gemini-3-flash-preview",  "provider": "gemini",  "rpm": 10, "cost_in": 0.15, "cost_out": 0.60,  "tier": "standard"},
    {"name": "gemini-2.5-flash",        "provider": "gemini",  "rpm": 10, "cost_in": 0.15, "cost_out": 0.60,  "tier": "standard"},
    {"name": "gemini-2.5-pro",          "provider": "gemini",  "rpm": 5,  "cost_in": 1.25, "cost_out": 10.00, "tier": "premium"},
    {"name": "gemini-2.0-flash",        "provider": "gemini",  "rpm": 15, "cost_in": 0.10, "cost_out": 0.40,  "tier": "fast"},
    # Groq models (FREE) â€” best for structured code output
    {"name": "openai/gpt-oss-120b",                  "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-coding"},
    {"name": "openai/gpt-oss-20b",                   "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-fast"},
    {"name": "moonshotai/kimi-k2-instruct-0905",     "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-coding"},
    {"name": "meta-llama/llama-4-maverick-17b-128e-instruct", "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-coding"},
    {"name": "llama-3.3-70b-versatile",              "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-general"},
    {"name": "qwen/qwen3-32b",                       "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-reasoning"},
    {"name": "llama-3.1-8b-instant",                 "provider": "groq", "rpm": 30, "cost_in": 0.0, "cost_out": 0.0, "tier": "free-fast"},
    # OpenAI models â€” used as escalation when Gemini is rate-limited
    {"name": "gpt-5.2-codex",     "provider": "openai", "rpm": 30, "cost_in": 1.75, "cost_out": 14.00, "tier": "openai-premium"},
    {"name": "gpt-4.1-nano",      "provider": "openai", "rpm": 30, "cost_in": 0.10, "cost_out": 0.40, "tier": "openai-fast"},
]

# Role-based model cascade â€” ordered by preference per role
ROLE_CASCADES = {
    "Orchestrator": [
        "gemini-3-pro-preview",
        "gemini-2.5-pro",
        "gpt-5.2-codex",        # OpenAI fallback for rate limits
        "qwen/qwen3-32b",
        "gemini-2.5-flash",
        "gemini-2.0-flash",
    ],
    "Developer": [
        "openai/gpt-oss-120b",                           # Best free coding model â€” OpenAI-grade structured output
        "moonshotai/kimi-k2-instruct-0905",               # Strong coding, good JSON adherence
        "meta-llama/llama-4-maverick-17b-128e-instruct",  # 128 experts, smarter than Scout
        "gpt-5.2-codex",                                # OpenAI fallback â€” best at code
        "gemini-2.5-flash",
        "openai/gpt-oss-20b",
        "gemini-2.0-flash",
        "gpt-4.1-nano",                                   # Cheapest OpenAI fallback
    ],
    "Reviewer": [
        "openai/gpt-oss-120b",
        "qwen/qwen3-32b",
        "gemini-2.5-flash",
        "moonshotai/kimi-k2-instruct-0905",
    ],
    "Tester": [
        "openai/gpt-oss-120b",
        "moonshotai/kimi-k2-instruct-0905",
        "meta-llama/llama-4-maverick-17b-128e-instruct",
        "gemini-2.5-flash",
        "gemini-2.0-flash",
        "openai/gpt-oss-20b",
    ],
}

# Default cascade for unknown roles (including dynamic agents)
DEFAULT_CASCADE = [
    "openai/gpt-oss-120b",
    "moonshotai/kimi-k2-instruct-0905",
    "gpt-5.2-codex",
    "gemini-2.5-flash",
    "qwen/qwen3-32b",
    "gemini-2.0-flash",
    "openai/gpt-oss-20b",
    "gpt-4.1-nano",
]

# Default cost estimates (for Gemini models used in budget tracking)
COST_PER_1M_INPUT = 0.15
COST_PER_1M_OUTPUT = 0.60


@dataclass
class TokenUsage:
    """Tracks token usage per agent and globally."""
    input_tokens: int = 0
    output_tokens: int = 0

    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens

    @property
    def estimated_cost(self) -> float:
        return (
            (self.input_tokens / 1_000_000) * COST_PER_1M_INPUT
            + (self.output_tokens / 1_000_000) * COST_PER_1M_OUTPUT
        )

    def to_dict(self) -> dict:
        return {
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "total_tokens": self.total_tokens,
            "estimated_cost_usd": round(self.estimated_cost, 4),
        }


class ModelRouter:
    """
    Multi-provider LLM router with role-based model selection.
    Drop-in replacement for GeminiClient â€” same generate() interface.
    """

    def __init__(
        self,
        gemini_api_key: str = "",
        groq_api_key: str = "",
        openai_api_key: str = "",
        max_retries: int = 3,
    ):
        # Initialize providers
        self._providers = {}

        self._gemini = GeminiProvider(gemini_api_key)
        if self._gemini.is_available:
            self._providers["gemini"] = self._gemini
            logger.info("âœ… Gemini provider initialized")

        self._groq = GroqProvider(groq_api_key)
        if self._groq.is_available:
            self._providers["groq"] = self._groq
            logger.info("âœ… Groq provider initialized (FREE models available)")

        self._openai = OpenAIProvider(openai_api_key)
        if self._openai.is_available:
            self._providers["openai"] = self._openai
            logger.info("âœ… OpenAI provider initialized (GPT-5.2-codex / 4.1-nano available)")

        if not self._providers:
            logger.error("âŒ No LLM providers available! Set at least GEMINI_API_KEY.")

        # For backward compat â€” file_context uses this
        self.client = self._gemini.client if self._gemini.is_available else None

        self.max_retries = max_retries
        self._queue_lock = asyncio.Lock()
        self._global_usage = TokenUsage()
        self._agent_usage: dict[str, TokenUsage] = {}
        self._current_model: str = ""

        # File context â€” injected file parts from Gemini Files API
        self._file_context = None

        # Budget cap
        self._budget_limit_usd: float = 1.00
        self._budget_warning_sent: bool = False
        self._budget_exceeded: bool = False
        self._on_budget_event = None

        # Per-agent failure escalation tracking
        # After ESCALATION_THRESHOLD consecutive failures, route to premium model
        self._agent_failures: dict[str, int] = {}
        self.ESCALATION_THRESHOLD = 3
        self.ESCALATION_MODEL = "gemini-3-pro-preview"

        # Initialize model states
        self._models: dict[str, ModelState] = {}
        self._model_config: dict[str, dict] = {}
        for m in MODELS:
            provider_name = m["provider"]
            # Only register models whose provider is available
            if provider_name in self._providers:
                self._models[m["name"]] = ModelState(
                    name=m["name"],
                    provider=provider_name,
                    rpm_limit=m["rpm"],
                    cost_in=m.get("cost_in", 0),
                    cost_out=m.get("cost_out", 0),
                )
                self._model_config[m["name"]] = m

        # Set initial active model
        first_available = next(iter(self._models), None)
        self._current_model = first_available or "none"
        logger.info(f"ğŸ“¡ Model router ready â€” {len(self._models)} models across {len(self._providers)} providers")

    def _pick_best_model(self, role: str = "", agent_id: str = "", model_override: Optional[str] = None) -> Optional[str]:
        """Pick the best available model for the given agent role.
        If model_override is set, try that model first.
        If the agent has hit the failure escalation threshold, override to premium."""
        cascade = list(ROLE_CASCADES.get(role, DEFAULT_CASCADE))

        # MODEL OVERRIDE: If agent is pinned to a specific model, try it first
        if model_override and model_override in self._models:
            pinned = self._models[model_override]
            if pinned.has_capacity:
                if model_override != self._current_model:
                    logger.info(f"ğŸ“Œ [{agent_id}] Pinned to {model_override}")
                self._current_model = model_override
                return model_override
            else:
                logger.warning(f"ğŸ“Œ [{agent_id}] Pinned model {model_override} exhausted, falling back to cascade")

        # ESCALATION: If agent has too many consecutive failures, lead with premium
        failures = self._agent_failures.get(agent_id, 0)
        if failures >= self.ESCALATION_THRESHOLD and self.ESCALATION_MODEL in self._models:
            logger.warning(
                f"ğŸ”¥ [{agent_id}] {failures} consecutive failures â€” ESCALATING to {self.ESCALATION_MODEL}"
            )
            # Put premium model first, keep rest as fallback
            if self.ESCALATION_MODEL in cascade:
                cascade.remove(self.ESCALATION_MODEL)
            cascade.insert(0, self.ESCALATION_MODEL)

        for model_name in cascade:
            if model_name in self._models:
                state = self._models[model_name]
                if state.has_capacity:
                    if model_name != self._current_model:
                        provider = self._model_config.get(model_name, {}).get("provider", "?")
                        tier = self._model_config.get(model_name, {}).get("tier", "?")
                        logger.info(f"ğŸ”„ Routing to {model_name} [{provider}/{tier}]")
                    self._current_model = model_name
                    return model_name

        # Fallback: try ANY model with capacity
        for name, state in self._models.items():
            if state.has_capacity:
                logger.warning(f"âš ï¸ Fallback to {name} (no role-preferred model available)")
                self._current_model = name
                return name

        return None

    def _get_provider(self, model_name: str):
        """Get the provider instance for a model."""
        config = self._model_config.get(model_name, {})
        provider_name = config.get("provider", "gemini")
        return self._providers.get(provider_name)

    def _track_usage(self, agent_id: str, response):
        """Track token usage from response metadata."""
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            meta = response.usage_metadata
            input_t = getattr(meta, 'prompt_token_count', 0) or 0
            output_t = getattr(meta, 'candidates_token_count', 0) or 0

            self._global_usage.input_tokens += input_t
            self._global_usage.output_tokens += output_t

            if agent_id not in self._agent_usage:
                self._agent_usage[agent_id] = TokenUsage()
            self._agent_usage[agent_id].input_tokens += input_t
            self._agent_usage[agent_id].output_tokens += output_t

    async def generate(
        self,
        agent_id: str,
        system_prompt: str,
        messages: list[dict],
        temperature: float = 0.7,
        role: str = "",
        model_override: Optional[str] = None,
    ) -> dict:
        """
        Generate a response using the best available model for the agent's role.
        Automatically falls back across providers on rate limit.
        """
        if not self._providers:
            raise RuntimeError("No LLM providers initialized â€” set GEMINI_API_KEY or GROQ_API_KEY in .env")

        # Budget check
        self._check_budget(agent_id)

        last_error = None
        total_attempts = self.max_retries * len(self._models)

        for attempt in range(total_attempts):
            # Pick the best model for this role (escalation-aware)
            async with self._queue_lock:
                model_name = self._pick_best_model(role, agent_id=agent_id, model_override=model_override)

            if not model_name:
                min_wait = min(s.wait_time() for s in self._models.values())
                wait = max(min_wait, 1)
                logger.warning(f"All models exhausted, waiting {wait:.1f}s")
                await asyncio.sleep(wait)
                continue

            model_state = self._models[model_name]
            provider = self._get_provider(model_name)
            if not provider:
                model_state.record_error()
                continue

            try:
                response = await provider.generate(
                    model_name=model_name,
                    system_prompt=system_prompt,
                    contents=messages,
                    temperature=temperature,
                    file_context=self._file_context,
                )

                model_state.record_success()
                self._track_usage(agent_id, response)

                # Parse JSON response
                text = response.text.strip()

                # Strip markdown code fences if present
                if text.startswith("```"):
                    text = text.split("\n", 1)[-1].rsplit("```", 1)[0].strip()

                try:
                    result = json.loads(text)
                    provider_name = self._model_config.get(model_name, {}).get("provider", "?")
                    logger.info(f"[{agent_id}] âœ… {model_name} [{provider_name}] responded")
                    return result
                except json.JSONDecodeError:
                    return {"thinking": "", "action": "message", "params": {}, "message": text}

            except Exception as e:
                error_str = str(e)
                last_error = e

                if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str or "rate_limit" in error_str.lower():
                    model_state.record_rate_limit()
                    logger.warning(f"[{agent_id}] ğŸ”„ {model_name} rate-limited, trying next...")
                    continue

                elif "403" in error_str or "PERMISSION_DENIED" in error_str:
                    logger.error(f"[{agent_id}] ğŸ”‘ Auth error on {model_name}: {e}")
                    model_state.cooldown_until = time.time() + 600  # 10 min cooldown
                    continue  # try another model instead of crashing

                elif "404" in error_str or "NOT_FOUND" in error_str:
                    model_state.record_rate_limit()
                    model_state.cooldown_until = time.time() + 300
                    logger.warning(f"[{agent_id}] âš ï¸ {model_name} not found, cooling 5min...")
                    continue

                elif "400" in error_str or "INVALID_ARGUMENT" in error_str:
                    # Try without JSON mode (Gemini-specific)
                    if isinstance(provider, GeminiProvider):
                        logger.warning(f"[{agent_id}] âš ï¸ {model_name} rejected JSON mode, retrying...")
                        try:
                            response = await provider.generate_no_json_mode(
                                model_name=model_name,
                                system_prompt=system_prompt,
                                contents=messages,
                                temperature=temperature,
                                file_context=self._file_context,
                            )
                            model_state.record_success()
                            self._track_usage(agent_id, response)
                            text = response.text.strip()
                            if text.startswith("```"):
                                text = text.split("\n", 1)[-1].rsplit("```", 1)[0].strip()
                            try:
                                result = json.loads(text)
                                logger.info(f"[{agent_id}] âœ… {model_name} responded (no-JSON-mode)")
                                return result
                            except json.JSONDecodeError:
                                return {"thinking": "", "action": "message", "params": {}, "message": text}
                        except Exception as retry_e:
                            logger.error(f"[{agent_id}] Retry failed: {retry_e}")
                            model_state.record_error()
                            continue
                    else:
                        model_state.record_error()
                        continue

                elif "500" in error_str or "503" in error_str:
                    model_state.record_error()
                    wait = (2 ** (attempt % 3))
                    logger.warning(f"[{agent_id}] Server error on {model_name}, retrying in {wait}s")
                    await asyncio.sleep(wait)
                    continue

                else:
                    logger.error(f"[{agent_id}] LLM error ({model_name}): {e}")
                    model_state.record_error()
                    continue

        raise RuntimeError(f"Failed after exhausting all models and retries: {last_error}")

    # â”€â”€â”€ Backward Compatibility Properties â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @property
    def active_model(self) -> str:
        return self._current_model

    def get_model_states(self) -> list[dict]:
        """Get status of all models for the UI."""
        return [
            {
                "name": s.name,
                "provider": s.provider,
                "active": s.name == self._current_model,
                "has_capacity": s.has_capacity,
                "requests_in_window": s.requests_in_window,
                "rpm_limit": s.rpm_limit,
                "cooled_down": s.is_cooled_down,
                "cooldown_remaining": max(0, s.cooldown_until - time.time()),
                "tier": self._model_config.get(s.name, {}).get("tier", "unknown"),
                "cost_in": s.cost_in,
                "cost_out": s.cost_out,
            }
            for s in self._models.values()
        ]

    def get_global_usage(self) -> dict:
        return self._global_usage.to_dict()

    def get_agent_usage(self, agent_id: str) -> dict:
        if agent_id in self._agent_usage:
            return self._agent_usage[agent_id].to_dict()
        return TokenUsage().to_dict()

    def get_all_usage(self) -> dict:
        return {
            "global": self.get_global_usage(),
            "agents": {
                aid: usage.to_dict()
                for aid, usage in self._agent_usage.items()
            },
            "active_model": self._current_model,
            "models": self.get_model_states(),
            "budget": self.get_budget_status(),
            "providers": list(self._providers.keys()),
        }

    # â”€â”€â”€ Budget Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def set_budget(self, limit_usd: float):
        self._budget_limit_usd = limit_usd
        self._budget_exceeded = False
        self._budget_warning_sent = False
        logger.info(f"Budget set to ${limit_usd:.2f}")

    def get_budget_status(self) -> dict:
        cost = self._global_usage.estimated_cost
        limit = self._budget_limit_usd
        pct = (cost / limit * 100) if limit > 0 else 0
        return {
            "limit_usd": round(limit, 2),
            "spent_usd": round(cost, 4),
            "remaining_usd": round(max(0, limit - cost), 4),
            "percent_used": round(min(pct, 100), 1),
            "exceeded": self._budget_exceeded,
            "warning": self._budget_warning_sent,
        }

    def _check_budget(self, agent_id: str):
        if self._budget_limit_usd <= 0:
            return
        cost = self._global_usage.estimated_cost
        pct = cost / self._budget_limit_usd

        if pct >= 1.0 and not self._budget_exceeded:
            self._budget_exceeded = True
            logger.warning(f"ğŸš¨ Budget EXCEEDED: ${cost:.4f} / ${self._budget_limit_usd:.2f}")
            raise BudgetExhaustedError(
                f"Budget limit of ${self._budget_limit_usd:.2f} exceeded (spent ${cost:.4f})"
            )
        if pct >= 0.8 and not self._budget_warning_sent:
            self._budget_warning_sent = True
            logger.warning(f"âš ï¸ Budget WARNING: ${cost:.4f} / ${self._budget_limit_usd:.2f} (80%)")

    def set_budget_callback(self, callback):
        self._on_budget_event = callback

    def set_file_context(self, file_context):
        self._file_context = file_context

    # â”€â”€â”€ Failure Escalation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def record_agent_failure(self, agent_id: str):
        """Record a consecutive action failure for an agent."""
        self._agent_failures[agent_id] = self._agent_failures.get(agent_id, 0) + 1
        count = self._agent_failures[agent_id]
        if count == self.ESCALATION_THRESHOLD:
            logger.warning(
                f"ğŸ”¥ [{agent_id}] Hit {count} failures â€” next request will escalate to {self.ESCALATION_MODEL}"
            )
        elif count > self.ESCALATION_THRESHOLD:
            logger.info(f"ğŸ”¥ [{agent_id}] Failure #{count} â€” still escalated to premium")

    def record_agent_success(self, agent_id: str):
        """Reset failure count on successful action."""
        prev = self._agent_failures.get(agent_id, 0)
        if prev >= self.ESCALATION_THRESHOLD:
            logger.info(f"âœ… [{agent_id}] Success after escalation â€” de-escalating back to normal routing")
        self._agent_failures[agent_id] = 0

    def get_escalation_status(self) -> dict:
        """Get current escalation status for all agents."""
        return {
            agent_id: {
                "failures": count,
                "escalated": count >= self.ESCALATION_THRESHOLD,
                "threshold": self.ESCALATION_THRESHOLD,
            }
            for agent_id, count in self._agent_failures.items()
            if count > 0
        }


# Backward compatibility alias
GeminiClient = ModelRouter
